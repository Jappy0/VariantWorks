#!/usr/bin/env python
#
# Copyright 2020 NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""A sample program highlighting usage of VariantWorks SDK to write a
simple SNP variant caller using a CNN.
"""

import argparse
import os

import nemo
from nemo import logging
from nemo.backends.pytorch.common.losses import CrossEntropyLossNM
from nemo.backends.pytorch.torchvision.helpers import compute_accuracy, eval_epochs_done_callback, eval_iter_callback
import torch

from variantworks.dataloader import ReadPileupDataLoader
from variantworks.io.vcfio import VCFReader
from variantworks.networks import AlexNet
from variantworks.sample_encoder import PileupEncoder, ZygosityLabelEncoder, ZygosityLabelDecoder
from variantworks.result_writer import VCFResultWriter


def create_pileup_encoder_and_model():
    # Create encoder for variant. Currently encoding only read and base quality in pileup.
    # More encoding layers supported. Please take a look at api docs for more info.
    encoding_layers = [PileupEncoder.Layer.READ,
                       PileupEncoder.Layer.BASE_QUALITY]
    pileup_encoder = PileupEncoder(
        window_size=100, max_reads=100, layers=encoding_layers)

    # Crete label encoder
    zyg_encoder = ZygosityLabelEncoder()

    # Neural Network
    alexnet = AlexNet(num_input_channels=len(
        encoding_layers), num_output_logits=3)

    return pileup_encoder, zyg_encoder, alexnet


def train(args):
    # Train a sample model with test data

    # Create neural factory as per NeMo requirements.
    nf = nemo.core.NeuralModuleFactory(
        placement=nemo.core.neural_factory.DeviceType.GPU)

    pileup_encoder, zyg_encoder, model = create_pileup_encoder_and_model()

    # Generate dataset
    bam = args.bam
    train_files = []
    for train_tp in args.train_tp:
        train_files.append(VCFReader.VcfBamPath(
            vcf=train_tp, bam=bam, is_fp=False))
    for train_fp in args.train_fp:
        train_files.append(VCFReader.VcfBamPath(
            vcf=train_fp, bam=bam, is_fp=True))
    vcf_loader = VCFReader(train_files)
    print("Train dataset size ", len(vcf_loader))

    # Create train DAG
    train_dataset = ReadPileupDataLoader(ReadPileupDataLoader.Type.TRAIN, vcf_loader, batch_size=32,
                                         shuffle=True, num_workers=args.threads, sample_encoder=pileup_encoder, label_encoder=zyg_encoder)
    vz_ce_loss = CrossEntropyLossNM(logits_ndim=2)
    vz_labels, encoding = train_dataset()
    vz = model(encoding=encoding)
    vz_loss = vz_ce_loss(logits=vz, labels=vz_labels)

    callbacks = []

    # Logger callback
    loggercallback = nemo.core.SimpleLossLoggerCallback(
        tensors=[vz_loss],
        step_freq=5,
        print_func=lambda x: logging.info(f'Train Loss: {str(x[0].item())}'),
    )
    callbacks.append(loggercallback)

    # Checkpointing models through NeMo callback
    checkpointcallback = nemo.core.CheckpointCallback(
        folder=args.model_dir,
        load_from_folder=None,
        # Checkpointing frequency in steps
        step_freq=-1,
        # Checkpointing frequency in epochs
        epoch_freq=1,
        # Number of checkpoints to keep
        checkpoints_to_keep=1,
        # If True, CheckpointCallback will raise an Error if restoring fails
        force_load=False
    )
    callbacks.append(checkpointcallback)

    # Get eval datasets
    eval_files = []
    for eval_tp in args.eval_tp:
        eval_files.append(VCFReader.VcfBamPath(
            vcf=eval_tp, bam=bam, is_fp=False))
    for eval_fp in args.eval_fp:
        eval_files.append(VCFReader.VcfBamPath(
            vcf=eval_fp, bam=bam, is_fp=True))

    # Create eval DAG if eval files are available
    if eval_files:
        eval_vcf_loader = VCFReader(eval_files)
        print("Eval dataset size ", len(eval_vcf_loader))
        eval_dataset = ReadPileupDataLoader(ReadPileupDataLoader.Type.EVAL, eval_vcf_loader, batch_size=32,
                                            shuffle=False, num_workers=args.threads, sample_encoder=pileup_encoder, label_encoder=zyg_encoder)
        eval_vz_ce_loss = CrossEntropyLossNM(logits_ndim=2)
        eval_vz_labels, eval_encoding = eval_dataset()
        eval_vz = model(encoding=eval_encoding)
        eval_vz_loss = eval_vz_ce_loss(logits=eval_vz, labels=eval_vz_labels)

        # Add evaluation callback
        evaluator_callback = nemo.core.EvaluatorCallback(
            eval_tensors=[eval_vz_loss, eval_vz, eval_vz_labels],
            user_iter_callback=eval_iter_callback,
            user_epochs_done_callback=eval_epochs_done_callback,
            eval_step=50,
            eval_at_start=False,
        )
        callbacks.append(evaluator_callback)

    # Invoke the "train" action.
    nf.train([vz_loss],
             callbacks=callbacks,
             optimization_params={"num_epochs": args.epochs, "lr": 0.0001},
             optimizer="adam")


def infer(args):
    # Inference

    # Create neural factory
    nf = nemo.core.NeuralModuleFactory(
        placement=nemo.core.neural_factory.DeviceType.GPU)

    pileup_encoder, _, model = create_pileup_encoder_and_model()

    # Generate dataset
    bam = args.bam
    test_files = []
    for test_file in args.test_file:
        test_files.append(VCFReader.VcfBamPath(
            vcf=test_file, bam=bam, is_fp=True))
    vcf_loader = VCFReader(test_files)

    # Create inference DAG
    test_dataset = ReadPileupDataLoader(
        ReadPileupDataLoader.Type.TEST, vcf_loader, batch_size=32, shuffle=False, sample_encoder=pileup_encoder, num_workers=args.threads)
    encoding = test_dataset()
    vz = model(encoding=encoding)

    # Invoke the "infer" action.
    inferred_zygosity = []
    zyg_decoder = ZygosityLabelDecoder()
    results = nf.infer([vz], checkpoint_dir=args.model_dir, verbose=True)

    # After inference results are available, convert inference logit output classes
    # to variant types using decoder.
    for tensor_batches in results:
        for batch in tensor_batches:
            predicted_classes = torch.argmax(batch, dim=1)
            inferred_zygosity += [zyg_decoder(pred)
                                  for pred in predicted_classes]

    # Finally write out inference results into a VCF with the predicted zygosities.
    result_writer = VCFResultWriter(
        vcf_loader, inferred_zygosities=inferred_zygosity, output_location=args.output_dir)
    result_writer.write_output()


def build_parser():
    def add_common_required_opts(parser):
        import multiprocessing
        parser.add_argument("-t", "--threads", type=int,
                            help="Threads to use for parallel loading.",
                            required=False, default=multiprocessing.cpu_count())
        parser.add_argument("--bam", type=str,
                            help="Path to BAM file for reads", required=True)
        parser.add_argument("--model_dir", type=str,
                            help="Directory for storing trained model checkpoints. Stored after every eppoch of training.",
                            required=False, default="./models")

    parser = argparse.ArgumentParser(
        description="Simple SNP caller based on VariantWorks.")
    subparsers = parser.add_subparsers(help="sub-command help", dest='command')

    train_parser = subparsers.add_parser(
        'train', help="Sub command for training simple SNP variant caller")
    add_common_required_opts(train_parser)
    train_parser.add_argument("--train_tp", nargs="+",
                              help="List of true positive VCF files for training.",
                              required=True)
    train_parser.add_argument("--train_fp", nargs="+",
                              help="List of false positive VCF files for training.",
                              required=True)
    train_parser.add_argument("--eval_tp", nargs="+",
                              help="List of true positive VCF files for evaluation.",
                              required=False, default=[])
    train_parser.add_argument("--eval_fp", nargs="+",
                              help="List of false positive VCF files for evaluation.",
                              required=False, default=[])
    train_parser.add_argument("--epochs", type=int,
                              help="Epochs for training.",
                              required=False, default=1)

    infer_parser = subparsers.add_parser(
        'infer', help="Sub command for inferencing using trained model.")
    add_common_required_opts(infer_parser)
    infer_parser.add_argument("--test_file", nargs="+",
                              help="List of VCF files to infer on.",
                              required=True)
    infer_parser.add_argument("-o", "--output_dir", type=str,
                              help="Output directory for inferred VCF files.",
                              required=False, default="./outdir")

    return parser


def main():
    parser = build_parser()
    args = parser.parse_args()
    if args.command == 'train':
        train(args)
    elif args.command == 'infer':
        infer(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
